\section{Sim-to-Real Transfer Techniques}

Researchers have developed a variety of techniques to enable policies learned in simulation to work in the real world. Figure 1 illustrates three common strategies: system identification (making the sim more like reality), domain adaptation (making the data or model translate between sim and real), and domain randomization (exposing the policy to diverse simulated experiences so it generalizes to real). We will discuss each in detail, including how they are implemented and examples of their use in RL. It is important to note that these techniques are often complementary – for example, one might first apply system identification to get a reasonably accurate simulator, then use domain randomization during training, and perhaps a form of adaptation at deployment. The optimal approach can depend on the task and what aspects of the sim-to-real gap are most significant (e.g. visual differences vs. physics differences).

\subsection{Domain Randomization}
\textbf{Concept}: Domain randomization (DR) is a strategy where, instead of trying to make the simulator perfectly realistic, we intentionally randomize various aspects of the simulation during training. The idea, introduced by Sadeghi and Levine (2016) and Tobin et al. (2017), is that by exposing the RL agent to a wide range of environment variations, the learned policy will focus on task-relevant features and become robust to changes. If the randomization is sufficiently broad, the real world – with its particular configuration – will appear to the agent as just another variation it has seen in training. In effect, domain randomization enlarges the training domain to encompass the real domain.

\textbf{What to Randomize}: Practically any parameter of the simulation that could differ in reality is a candidate for randomization. Common randomizations include: object colors and textures, lighting conditions, background imagery, positions and shapes of objects, sensor noise, and physics properties like masses, friction coefficients, and joint damping. For example, in a simulated robotic vision
task, one might randomize the colors and textures of walls and floors each episode, add random noise or blur to the camera images, and randomize lighting direction and intensity. In a dynamics-centric task like robot locomotion, one might randomize gravity slightly, vary the robot’s motor torque scalars, or add random forces (perturbations) during the episode to simulate bumps or wind. The randomization
ranges should be chosen to cover the plausible real-world variations – often initially set wide when unsure. Notably, domain randomization does not attempt to perfectly mimic reality; it instead says “throw everything at the agent, and if it can handle all of it, it will handle reality.”

\textbf{Effect on Learning}: During training, each episode or training iteration uses a differently randomized environment. The RL algorithm, say Proximal Policy Optimization (PPO), optimizes the policy to maximize expected reward over this distribution of environments (rather than one fixed environment). Mathematically, if $\xi$ denotes a vector of simulation parameters to randomize (both visual and physical), domain randomization seeks a policy $\pi_\theta$ that performs well under the expectation over $\xi \sim \Xi$, where $\Xi$ is the space of randomization. In other words, the objective
becomes:
$$\max_\theta \; \mathbb{E}{\xi \sim \Xi}\left[\, \mathbb{E}[R\tau]\right],(\xi)$$
where $R(\tau)$ is the cumulative reward of a trajectory $\tau$ sampled from the simulator with randomization $\xi$. By optimizing this, the agent learns to handle all the randomized conditions, rather than overfitting to one setting.

In pseudocode, a domain randomization training loop might look like this:

\begin{lstlisting}[mathescape=true]
initialize policy parameters $\theta$
for iteration = 1 to N:
    random parameters $\xi$ from distribution $\Xi$
    set simulator environment with $\xi$ (random textures, physics, etc.)
    run policy $\pi_\theta$ in simulator for one episode and collect reward R
    update $\theta$ using RL algorithm (e.g. policy gradient) to maximize R
end for
\end{lstlisting}
This way, each training episode occurs in a slightly different simulated world. By the end of training, the policy has effectively seen many “worlds.” Ideally, when deployed in the real world (which is like one specific set of parameters $\xi_{real}$), the policy treats it as just another instance and still performs well.

\textbf{Examples and Successes}: Domain randomization has enabled several notable sim-to-real successes in RL and robotics. Sadeghi and Levine’s pioneering work (CAD2RL, 2016) trained a drone’s collision avoidance policy entirely in a simulator with randomized textures and lighting on virtual indoor hallways. The resulting policy was able to fly a real quadrotor down hallways and avoid obstacles \textit{without seeing a single real image during training} – a remarkable feat at the time. Similarly, Tobin et al. (2017) randomized the appearance of objects and backgrounds in a simulator and trained a deep neural network to locate objects; when tested on a real robot, the vision system achieved ~1.5 cm accuracy in object positioning and enabled a robot arm to grasp objects in clutter using only the policy trained on synthetic data. This was one of the first demonstrations of zero-shot sim-to-real transfer for a deep neural network policy (no real fine-tuning), proving the power of aggressive randomization.

Later works extended domain randomization to more complex tasks. OpenAI’s \textbf{Dactyl} system (2018) is a famous example: they trained a robotic hand to dexterously reorient a block and even solve a Rubik’s Cube using only simulation with extensive randomizations. They randomized physical properties like friction, object mass, and joint damping, as well as visual factors, in a MuJoCo simulator. Thanks to this, the learned policy could be deployed directly on the real Shadow Hand robot, achieving the task without any real-world training. Figure 2 (left) illustrated the kind of visual diversity the network saw during training – which helped it not to rely on any particular texture or color. In the realm of locomotion, domain (dynamics) randomization has been applied to legged robots: for instance, adding random forces or altering ground friction during training can produce a policy that keeps a real quadruped robot stable on different terrains and under perturbations. We will detail a locomotion case in Section IV.

\textbf{Limitations}: While domain randomization is powerful, it is not without drawbacks. First, if the randomization ranges are chosen improperly, the policy might either overfit to unrealistic scenarios or not generalize enough. Too little randomization, and the policy still gets surprised by reality; too much randomization, and learning can become harder or the policy might adopt an overly conservative strategy that works “okay” everywhere but not great for the real task. For example, if you randomize physics constants over a very wide range, the agent might learn a very cautious control that is suboptimal for the actual robot. Tuning the randomization distribution $\Xi$ often requires some guesswork or iterative refinement (possibly incorporating a bit of real data to see where the simulator diverges most). Another limitation is that domain randomization doesn’t \textit{eliminate} the reality gap – it sidesteps it by brute-force generalization. In some cases, extremely large neural network policies or long training times are needed to succeed across all variations. OpenAI’s Dactyl, for instance, used hundreds of years of simulated experience collected across thousands of parallel simulations to learn its policy. Such scale may be prohibitive for many researchers. Despite these challenges, DR remains one of the most widely used and empirically successful approaches for sim-to-real in RL, especially when dealing with visual differences or when collecting even a small amount of real training data is difficult.

\subsection{Domain Adaptation}
\textbf{Concept}: Domain adaptation techniques aim to \textit{explicitly bridge the gap between the source domain (simulation) and the target domain (real world)} by adapting either the data or the learned representations. Unlike domain randomization, which trains on raw simulation data and hopes the policy generalizes, domain adaptation tries to modify things so that the simulation “looks more like” reality (or vice versa) or that the policy is encouraged to be invariant to the domain differences. This is commonly achieved through machine learning models that learn a transformation between simulated and real data distributions. In the context of RL sim-to-real, the most typical use case is adapting \textit{visual observations}: e.g., using a neural network (often a GAN – Generative Adversarial Network) to translate simulated images into more realistic images before feeding them to the agent, or conversely, mapping real images into the simulator’s image style for processing. The goal is to remove the \textit{visual domain shift} so that an agent sees a more consistent observation space between training and deployment.

\textbf{Approaches}: One class of domain adaptation methods is \textit{pixel-level adaptation}. For example, imagine you have a policy that was trained in a simulator using synthetic camera images. At deployment, instead of directly giving real camera images to the policy (which might cause it to fail due to different colors/textures), you first pass each real image through a translator network that was trained to make real images look “simulated” or make simulated images look “real.” A popular tool for this is CycleGAN or related GAN architectures, which can learn to translate images from one domain to another while preserving content. GraspGAN (2018) is an example where a GAN was trained to translate simulated robotic grasping images into realistic images, effectively correcting some of the rendering gap. By augmenting the training with these more realistic “fake real” images, the grasping policy’s success on the real robot improved significantly compared to using raw simulator images. The limitation of a naive GAN approach is that it might alter details that are important for the task (since it’s purely trying to fool a discriminator into thinking the image is real). For instance, a GAN might slightly shift an object’s position or shape as a side effect of making lighting more realistic, which could confuse a control policy.

To address this, advanced methods incorporate \textit{task-awareness} into the adaptation. \textbf{RL-CycleGAN} (Rao et al., 2020) is one such approach: it extends CycleGAN with an additional loss that ensures the adapted images do not change the RL agent’s predicted Q-values for the scene. In other words, it encourages the translation to be \textit{content-preserving for task-relevant features}. In their setup, a simulated image is translated to a “realistic” image, fed through the agent’s network, and also the original is fed – the agent’s decisions on both should match. This guided the GAN to only change superficial aspects (like textures or lighting) but not move objects around or remove them, resulting in a consistent perception for the agent. By using RL-CycleGAN, the authors achieved state-of-the-art results in sim-to-real robotic grasping and door opening tasks, outperforming prior adaptation methods.

Another angle is \textit{feature-level adaptation}: instead of translating raw images, one can train the neural network to learn an intermediate representation that is invariant to the domain. For example, using techniques like \textit{adversarial domain confusion}, a network is trained with a loss that makes its internal features unable to discriminate between sim and real data, while still being useful for the task. This way, the high-level features are domain-agnostic. Some approaches add a domain classifier to the network and use adversarial training (à la GAN) to make the feature extractor fool the domain classifier – a method used in unsupervised domain adaptation in computer vision. In robotics, Bousmalis et al. (2018) introduced \textit{SimOpt} and other frameworks where a small amount of real data is used to adapt the network’s representations or fine-tune the policy in an unsupervised manner.

\textbf{When and Why}: Domain adaptation is particularly useful when the difference between simulation and reality is largely in the sensory data (e.g., images) rather than the underlying dynamics. If a policy’s failure is mostly due to, say, the difference in image appearance (color, lighting, backgrounds), and not due to physics, then an image-to-image translation or similar technique can be highly effective. Adaptation can also leverage real data if available: for example, one might use a set of unlabeled real images from the robot’s camera to help train the GAN or domain adaptation model, even if those images weren’t gathered by the policy itself (this is unsupervised adaptation because we don’t necessarily require real action or reward data, just raw observations). This is advantageous when one can easily collect some images of the real environment without risk (e.g., moving the robot through random motions or even taking pictures manually), to inform the adaptation model of what real visuals look like.

It’s worth noting that domain adaptation is not limited to visual modalities. One could adapt other sensor data or even dynamics. For instance, one could imagine adapting simulated depth sensor readings to match real depth sensor noise characteristics. In practice, visual adaptation has been the most studied, often combined with policies that use image inputs.

\textbf{Limitations}: The downside of domain adaptation approaches is that they add complexity – an additional model (like a GAN) to train, and potential failure modes if the adaptation isn’t perfect. A poorly trained image translator could introduce its own artifacts that confuse the policy. There is also an assumption that the real and sim data distributions have enough overlap that a translation is possible; if the real scenes are vastly different (beyond just style differences), adaptation might not suffice. Moreover, many adaptation methods require at least some real data (even if unlabeled) to train the translators or domain classifiers. In truly zero-real-data scenarios, one must rely on simulated data alone – here domain randomization might be preferred. However, one can combine adaptation with randomization: e.g., use domain randomization to produce a variety of sim images and then train a GAN to map that variety to real-looking images. This was done in some works to cover even more diversity.

In summary, domain adaptation provides a powerful set of tools for aligning the simulated domain with the real domain at the data or feature level. By doing so, it reduces the burden on the policy to generalize and directly attacks the reality gap. When successful, it can significantly improve sim-to-real transfer, as evidenced by methods like GraspGAN and RL-CycleGAN enabling higher success rates in real-world robotic tasks. As we will see in case studies, adaptation is often used in vision-based tasks like grasping and navigation, sometimes in tandem with randomization.

\subsection{System Identification and Simulator Calibration}
\textbf{Concept}: \textit{System identification} (SysID) involves empirically measuring and tuning the simulator to more accurately reflect the real world. In the context of sim-to-real for RL, system identification means using real-world data to adjust the simulator’s parameters (physical properties, sensor models, etc.) so that the simulated behavior aligns closely with the real robot’s behavior. If successful, this approach narrows the sim-to-real gap by making the simulation itself as realistic as possible, thereby reducing the burden on the policy to handle differences. In contrast to domain randomization or adaptation, system identification is typically done \textit{before or during training}, to set up a better simulation, rather than at runtime or after a policy is trained (though there are also online system ID techniques).

How it works: The process often involves the following steps: (1) Perform controlled experiments on the real system to collect data. For example, one might command a robot joint to move with a certain voltage and record the actual motion, or drop a robot arm from a known position to measure its damping and friction. (2) Define a set of simulator parameters that are uncertain (masses, friction, motor gains, etc.). (3) Use an optimization method to find parameter values that make the simulator’s output match the real data as closely as possible. This could be done via manual tuning or automated techniques like least squares fitting, evolutionary algorithms, or Bayesian optimization. (4) Update the simulator with these calibrated parameters and validate that the simulator now predicts reality more accurately.

In robotics, system ID is a well-established field on its own. For RL sim-to-real, it’s common to identify the robot’s dynamics. For instance, Peng et al. (2018) in a work on robotic control transfer randomized dynamics during training and then used real data to select which dynamics were most plausible – effectively performing system ID in tandem with domain randomization. Tan et al. (2018) for quadruped locomotion (we will discuss shortly) improved their sim by building an accurate actuator model and simulating sensor latency. By measuring the motor responses and delays on the physical robot, they could simulate them, which standard simulators hadn’t done by default. This significantly closed the gap in that application.

Another powerful approach is iterative system identification: using policy rollouts. One method is to train a preliminary policy in a random or nominal simulator, test it on the real robot (cautiously), observe the discrepancies in behavior, and then adjust simulator parameters to fix those discrepancies. This can be iterated: essentially a loop of “sim -> real -> sim” updates. In fact, the term real-to-sim-to-real is sometimes used, where data from a brief real robot test is used to update the sim (real-to-sim), then the improved sim is used to further train the policy (sim-to-real). This was done in works like Chebotar et al. (2019) “Closing the Sim2Real Loop” where they used real trajectories to learn residual physics parameters.

System identification vs. Randomization: In Figure 1 (top-left vs. right), we see the philosophical difference. System ID tries to move the simulator’s parameters (the red dot) closer to the real parameters (green triangle), ideally overlapping them – a direct approach, but one that might not capture everything if the model form is incomplete. Domain randomization, by contrast, covers a broad range (dashed circles) hoping reality lies within it. The two can be combined: one can start with a calibrated simulator (via system ID) and then apply randomization around that calibrated point to cover remaining uncertainty. This often yields better results than either alone. 

Limitations: A challenge with system identification is that it can be labor-intensive and sometimes impossible to perfectly identify certain aspects. Careful calibration is necessary, but as Lilienberg (2019) notes, this can be expensive and some parameters vary over time or context (e.g., friction can change with temperature or wear). That means even after identification, the reality gap might creep back as conditions change. Additionally, one can only calibrate the aspects one knows to measure; there could be unmodeled dynamics that no amount of parameter tuning will fix (for example, flexible components when using a rigid-body simulator). Over-reliance on system ID might lead to false confidence in the simulator. 

Another issue is the complexity of the robot or environment. For multi-degree-of-freedom robots with contacts, identifying all contact properties and joint properties is a high-dimensional optimization problem that may require specialized system ID tools or substantial expertise. In some cases, researchers disassemble robots to measure physical properties for simulation – clearly not always feasible. There’s also the risk of damage or safety concerns when performing identification experiments on a physical system (though usually these tests are benign like moving joints slowly, etc.). 

Despite these challenges, system identification has proven very beneficial. In particular, for dynamics-critical tasks (like locomotion or precise control), having the simulator’s dynamics close to reality simplifies the RL agent’s job enormously. In one example, Tan et al. (2018) found that without an accurate actuator model, a quadruped policy learned in sim would fail on the real robot; with the model and latency identified, plus some dynamics randomization, the robot executed agile gaits successfully on hardware. In Section IV, we will describe this case in more detail. 

In summary, system identification attempts to minimize the reality gap at its source by making a better simulator. While it may not eliminate the gap entirely, it reduces it, and when used in combination with the other techniques (randomization to handle residual uncertainty, adaptation if needed for sensors), it contributes significantly to sim-to-real success.
