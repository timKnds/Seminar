\section{Background}
\subsection{Definition}
An RL task is an MDP characterized by observation (state) $s$, action $a$, transition $T(s,a\to s')$, and reward $r$. In simulation, these are often idealized: perfect sensors, discretized actions, deterministic physics, and easily computable rewards. In reality, by contrast, sensors produce noisy observations, actuators have latency and limits, dynamics are stochastic and hard to model, and rewards may be sparse or indirect. Table~\ref{tab:elements} summarizes typical sim-versus-real discrepancies. Any of these mismatches can cause failure: for example, a vision policy may overfit simulated textures and fail on real images, or an action sequence learned without actuator latency may be unsafe on hardware.

\begin{table}[htbp]
    \centering
    \caption{Simulated vs real MDP components: sources of discrepancy (simulated vs real).}
    \begin{tabular}{|p{0.3\linewidth}|p{0.6\linewidth}|}
        \hline
        \textbf{Observation:} & Idealized vs noisy vision/depth; synthetic vs real textures (rendering) \\
        \hline
        \textbf{Action:} & Precise, discrete commands vs actuator noise, latency, continuous control \\
        \hline
        \textbf{Transition:} & Deterministic physics vs stochastic dynamics, friction, contact variations \\
        \hline
        \textbf{Reward:} & Engineered or dense reward vs sparse/uncertain task objectives \\
        \hline
    \end{tabular}
    \label{tab:elements}
\end{table}

Each \simtoreal method typically addresses one or more of these elements. For instance, \emph{domain randomization} perturbs visuals and/or physics during training to cover a wide range of possible observations or transitions {\cite{Tobin2017,Sadeghi2017}}. In contrast, \emph{system identification} methods use real data to calibrate the simulator’s parameters {\cite{Chebotar2019}}. Some approaches combine both: e.g.\ OpenAI’s ADR gradually expands randomization range, effectively automating calibration while training {\cite{Akkaya2019}}. We now detail techniques by OATR component, starting with observations.

% These four elements together determine the behaviour of an RL agent. The \simtoreal challenge can be viewed as discrepancy in one or more of these elements between the simulator MDP and the real-world MDP. Each technique can be used to mitigate one or more type of discrepancy.