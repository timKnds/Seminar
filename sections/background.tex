\section{Background}

\subsection{Reinforcement Learning Basics}
In a standard reinforcement learning setup, an agent interacts with an environment over discrete time steps. At each time step $t$, the agent observes the state $s_t$ and chooses an action $a_t$ according to its policy $\pi$. The environment the transitions to a new state $s_{t+1}$ and emits a reward $r_t$ indicating the immediate outcome of the action. The agent's goal is to learn a policy that maximizes the expected cumulative reward over time. Formally, this is often modeled as a Markov Decision Process (MDP) defined by the tuple $(S, A, P, R)$ - state space, action space, transition, dynamics, and reward function. Modern RL algorithms (e.g. DQN, PPO, DDPG) use deep neural networks as function approximators to represent policies or value functions, allowing agents to tackle high-dimensional state spaces (like images from cameras) and learn complex behaviors.

A key requirement for successful RL is a large number of interactions with the environment to explore and learn an effective policy. Training directly on physical robots or real systems is often impractival due to time, cost, and safety constraints. For example, an autonomous drone learning via trial-and-error might crash many times before mastering flight, which is costly and dangerous. This is where \textit{simulation environments} become extremely useful. A simulator can emulate the real environment's dynamics and sensors, enabling the agent to gather experience rapidly and safely in a virtual world. The simulator can run faster than the real time and can be reset or configured arbitrarily, providing a rich supply of training data that would be infeasible collect on real hardware. 

Many popular RL environments are indeed simulations – from game engines to physics simulators. In robotics, physics engines such as \textbf{MuJoCo}, \textbf{PyBullet}, and \textbf{Gazebo} are widely used to simulate robots and their surroundings. For instance, MuJoCo (Multi-Joint dynamics with Contact) provides efficient rigid-body physics and is used for simulating robotic locomotion and manipulation. PyBullet (built on the Bullet physics engine) is an open-source simulator often used for robotics research, and Gazebo is commonly paired with ROS (Robot Operating System) for realistic 3D robot simulations, including sensors and environments. More recently, Isaac Gym (NVIDIA) introduced GPU-accelerated simulation, enabling thousands of simulation instances to run in parallel on a single GPU. This massively parallel simulation can speed up RL training by 2–3 orders of magnitude compared to traditional CPU simulators. With such simulation tools, an RL agent can effectively experience years’ worth of interactions in a matter of hours or days in wall-clock time.

\textbf{RL in Simulation}: When training in a simulator, we typically assume we have a reasonably accurate model of the environment’s dynamics. The agent is optimized on this source domain (the simulation) under the expectation that the learned policy will also perform well when deployed in the target domain (the real world). In an ideal scenario with a perfect simulator, an optimal policy in simulation would also be optimal in reality. In practice, however, no simulator is a perfect replica of the real world. Physics engines use simplifications: for example, simulating contact dynamics, friction, or sensor noise exactly as in reality is extremely difficult. Moreover, certain real-world factors like wear-and-tear, temperature effects, or unexpected disturbances are hard to model exhaustively. As a result, a policy may overfit to the simulator’s quirks and fail when those assumptions are violated in the real environment. The discrepancy between the simulator and reality is what we call the sim-to-real gap.

\subsection{The Sim-to-Real Gap}
The \textit{sim-to-real gap} (or reality gap) refers to the differences in dynamics, sensory inputs, and other factors between a simulated environment and the real world which can cause an RL agent's performance to drop when moving from sim to real. These differences can arise in various forms:
\begin{itemize}
    \item \textbf{Modeling errors}: The simulator may have incorrect physical parameters (mass, friction coefficients, motor torque curves, aerodynamics, etc.) or simplified dynamics. For example, a simulator might treat a robot's joints as perfectly rigid, whereas the real robot's joints have flexibilities or backlash. Similarly, simulated cameras might produce ideal images while real cameras suffer from motion blur or lens distortion. Even high-fidelity simulators cannot capture every nuance of reality (such as minor manufacturing differences between units of the same robot, or the way cables and batteries affect a robot’s weight distribution).
    \item \textbf{Sensor discrepancies}: In simulation, one can directly access the true state of the system (like precise object positions) or render synthetic images. In reality, sensors provide observations with noise, delay, and limited accuracy. Lighting conditions, reflections, or backgrounds in real images differ from the neatly rendered simulated visuals. An agent trained on pristine simulation images might be confused by the clutter and variability of real-world visual scenes if not properly accounted for.
    \item \textbf{Domain boundaries}: The real world may contain scenarios that were never present in the simulator. An RL policy trained in a simulated room might encounter novel textures, object shapes, or dynamics when applied in a real room, if the simulation did not encompass those variations. In other words, the real environment could lie outside the training distribution of the simulation. This is especially problematic if the simulator was tuned to be as “clean” and deterministic as possible for faster learning – the policy might exploit simulator-specific artifacts that do not hold in reality.
\end{itemize}
The consequence of the sim-to-real gap is often a dramatic drop in performance when a naive simulated-trained policy is tested on the real system. For instance, without special measures, a robot arm policy trained to grasp objects in a simulator might consistently miss the object in real trials,
or a drone’s flight controller might become unstable in actual flight due to aerodynamic effects not present in simulation. Researchers have documented that directly transferring deep RL policies from simulation to reality \textit{without adaptation} rarely succeeds except in very constrained cases. Thus, a major area of research focuses on techniques to \textbf{bridge the sim-to-real gap} so that the benefits of simulation (abundant data, safety, speed) can be leveraged without sacrificing real-world performance.

Several approaches have emerged to tackle this transfer problem. Broadly, these approaches either (1) make the simulator more like the real world, or (2) make the learned policy robust enough to handle differences, or (3) adapt the policy (or data) when moving to real. In the next section, we will delve into the three primary techniques – \textit{domain randomization}, \textit{domain adaptation}, and \textit{system identification} – that exemplify these strategies. Before that, it’s worth noting that these are not mutually exclusive; in fact, state-of-the-art sim-to-real results often combine multiple techniques. We will later see examples where improving simulator fidelity (system ID) is used together with training for robustness (randomization) to achieve successful transfer.