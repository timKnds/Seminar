\section{Observation-Level Transfer}
Observation discrepancies are the most visually apparent sim-to-real differences. These include divergences in visual appearance (textures, lighting, backgrounds), sensor noise patterns, and partial observability. There are two major families of methods address the observation (state) gap: domain randomization and domain adaptation, but also other approaches like sensor calibration or fusion can be used to align simulated observations with reality.

\subsection{Domain Randomization}
This technique randomizes the rendering of the simulator’s observations so that the RL agent is trained on a wide variety of appearances, forcing it to learn features invariant to those visual details. Pioneering work by Tobin et al. (2017) demonstrated that randomizing textures, colors, lighting, and camera angles in simulation enabled a vision-based object detector to generalize to real images, even though the policy never saw a single real photograph during training. Sadeghi and Levine’s CAD2RL experiment (2017) applied pure visual randomization to train a drone to fly in simulation using only RGB images; remarkably, the learned policy could control a real quadrotor through indoor corridors based solely on its onboard camera, achieving real-world collision avoidance without any real-image training data. In robotic manipulation, OpenAI’s Dactyl project used extensive visual domain randomization to train a robot hand to reorient objects. They randomized lighting, textures, and backgrounds in the virtual scene such that the hand’s camera observed many variations. This produced a policy that was resilient to real-world visual differences, allowing the real robot to manipulate a block and even solve a Rubik’s Cube despite substantial changes in lighting or object appearance. The general principle is that by exposing the agent to diverse simulated observations, it learns a representation that is robust to the exact appearance of the state, thus coping with real-world visuals it was never explicitly trained on. One caution is that unbounded randomization can make training unstable or overly difficult if the agent sees too much random variation too soon. To address this, researchers introduced curriculum strategies. For example, Automatic Domain Randomization (ADR) (OpenAI et al., 2019) begins with small random variations and progressively increases the randomness as the policy improves. ADR was used to great effect in training Dactyl’s vision and control policies – starting with near-realistic visuals and gradually adding more extreme randomizations (e.g., wildly colored or textured objects) once the agent mastered easier variants. This curriculum ensures the agent is not overwhelmed initially, and it focuses training on closing the sim-to-real gap incrementally. Recent studies like Yuan et al. (2024) have further formalized curriculum randomization to stabilize RL training and achieve state-of-the-art vision-based policy transfer. In summary, visual domain randomization treats every rendered frame as a new “domain,” so the agent essentially learns to ignore visual specifics and rely on task-relevant features, thereby mitigating observation mismatches between simulation and reality.

\subsection{Domain Adaptation}
While domain randomization modifies the simulator, domain adaptation typically uses data-driven post-processing or representation learning to bridge the gap from simulation outputs to real inputs (or vice versa). The idea is to align the feature distributions of simulated and real observations, so that a policy trained on simulation features will behave correctly on real features. One common approach is to use adversarial training: for instance, Bousmalis et al. (2018) employed a generative adversarial network to transform simulated camera images into a more realistic style before feeding them to a robotic grasping policy. By training a convolutional network to fool a discriminator into mistaking simulated observations for real, the feature representations become invariant to the origin (sim vs real). This enables the policy to perform consistently across domains, as shown in experiments where a grasping policy learned in sim (augmented by an adaptation network) could directly grasp real objects with much higher success rates than an unadapted policy. Another example is Pixel-Level Domain Adaptation by Ganin and colleagues, and the SimGAN framework (2017) which refined simulator-rendered images to look more photorealistic using unpaired image translation. Domain adaptation isn’t limited to vision: it can apply to any observation modality. For instance, aligning LiDAR point cloud distributions or calibrating simulator sensor noise models also fall under this category. Carlson et al. (2019) demonstrated sensor calibration for simulations of radar and cameras so that the statistical properties (e.g., noise covariance, artifact patterns) match those observed from real sensors. By minimizing the discrepancy in observations, the agent is less likely to be thrown off by unfamiliar inputs when deployed. Recent research has also explored self-supervised representation learning to find a shared latent state space for sim and real observations. Jeong et al. (2020) introduced a sequence-based autoencoder that learns latent states capturing the underlying environment dynamics, yielding improved sim-to-real performance in a robot manipulation task. The key takeaway is that domain adaptation techniques insert an adaptation layer or transform in the pipeline so that the observation fed into the policy looks as if it came from the same distribution as training data, even if it actually comes from a different domain. This can be achieved through adversarial loss, contrastive learning, or mapping functions between domains.

\subsection{Other Observation-Level Strategies}
In some cases, rather than rely on a single sensor modality, combining multiple observations can mitigate sim-to-real issues. For example, if a simulator provides perfect state information (like object positions) which are not available directly from sensors in reality, one could train a policy that uses both vision and this privileged state in sim, then at deployment use real vision plus perhaps another sensor that helps approximate the missing state. This idea of sensor fusion and complementary observations is highlighted by Mahajan et al. (2024), who show that integrating data from multiple sensors (e.g., camera + depth sensor) yields more robust policies. The intuition is that even if one sensor’s data differ between sim and real, the additional modalities provide redundancy. Another trend is leveraging large pretrained models – for instance, using a pretrained foundation model or pre-trained visual encoder that has seen many real images, to encode simulated observations into a representation that is already aligned with real-world visual features. By plugging such an encoder into the RL pipeline, the hope is that the agent’s perception is grounded in real-world priors, reducing the gap in how it interprets simulated vs real observations. While these approaches are emerging (e.g., using CLIP or other vision transformers to encode state), they point toward the same goal: ensuring the agent “sees” the simulator and the real world in a consistent way.

In summary, observation-level transfer techniques focus on the state/observation space $S$ of the MDP. Methods like visual domain randomization widen the training distribution to encompass reality, whereas domain adaptation methods explicitly learn mappings or invariant features to align sim with real observations. Both approaches have enabled zero- or few-shot transfers in vision-based tasks that were once thought to require real data. Notably, the success of CAD2RL and Dactyl underscore that bridging the perception gap is often the first and pivotal step in sim-to-real reinforcement learning.