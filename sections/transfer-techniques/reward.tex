\section{Reward-Level Transfer}
The reward function reflects the task objective and is often overlooked in \simtoreal discussions. In simulation, one can design dense rewards (e.g.\ penalize distance to goal at each step). In reality, measuring the true task success may be harder (e.g.\ it may require expensive sensors or human input). Furthermore, simulators sometimes use artificial knowledge (like exact pose or contact flags) to compute reward that are unavailable in real deployment.

A simple tactic is to avoid reward mismatch by using task goals that can be measured in both domains. For example, if the task is goal-reaching, use the distance of an end-effector (which can be tracked by real sensors) as the reward. If a reward requires internal state (like velocity in sim), one can approximate it using onboard sensors or drop it from the function. Another strategy is to degrade the reward signal during training (add noise or delays) to simulate real sensing imperfections.

Recent research increasingly leverages \textit{learned reward functions} to address the limitations of hand-crafted design. Instead of manually specifying the objective, reward models are trained from data, either through supervised learning or auxiliary objectives. Methods such as self-supervised reward learning \cite{Eysenbach2021} or consistency-based techniques \cite{Sharma2022} allow agents to generate internal learning signals without ground-truth rewards. These approaches have been applied successfully in tasks such as visual navigation and robotic manipulation.

\textbf{Inverse Reinforcement Learning (IRL)} provides a principled framework for inferring reward functions from expert behavior \cite{Ng2000}. Instead of defining a reward function directly, the agent is shown successful demonstrations and attempts to recover a reward that explains them. Recent methods such as Adversarial IRL (AIRL) \cite{Fu2018} model this as a game between policy and discriminator, improving scalability and generalization. Applications include robotic manipulation and autonomous driving. However, IRL methods often require large, high-quality demonstration datasets and are sensitive to noise in the trajectories.

\textbf{Preference-Based Reinforcement Learning (PbRL)} offers a more data-efficient alternative by using binary comparisons instead of full expert demonstrations. Instead of labeling trajectories with reward values, a human selects which of two behaviors is preferred. These comparisons are used to train a reward model, which is then optimized. Notable examples include D-REX \cite{Brown2019} and PEBBLE \cite{Lee2021}, which have shown strong results in sparse-reward environments like Atari and robotics. OpenAIâ€™s preference-based approach enabled a simulated agent to learn a backflip using fewer than 1{,}000 bits of human feedback \cite{Christiano2017}. Similarly, Deep TAMER \cite{Warnell2017} showed that just minutes of real-time interaction can outperform traditional RL in games like Bowling. Despite their efficiency, these methods are sensitive to feedback consistency and suffer from potential bias.

Compared to discrepancies in observations, actions, and transitions, reward-level gaps are more task-specific and harder to generalize. While fewer \simtoreal pipelines explicitly address reward transfer, its importance is growing as RL moves toward real-world deployment. Hybrid approaches that combine learned reward models, preference feedback, and limited real-world supervision are a promising direction.

\paragraph{Evaluation.} Reward design remains one of the most challenging aspects of sim-to-real transfer. Self-supervised and learned rewards help avoid reliance on inaccessible ground-truth signals but often lack interpretability. Inverse reinforcement learning (IRL) approaches, such as AIRL~\cite{Fu2018}, perform well when expert demonstrations are available but are difficult to scale and sensitive to trajectory quality.