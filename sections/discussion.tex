\section{Discussion and Outlook}
Using the OATR framework clarifies that \simtoreal transfer is multi-faceted. Domain randomization has emerged as a powerful unifying idea: it can be applied to observations (textures/colors), actions (motor noise), and transitions (dynamics parameters). Its strength is simplicity and applicability without real data\cite{Tobin2017,Sadeghi2017,Akkaya2019}. However, it lacks guarantees and may require careful tuning of parameter ranges. System identification and grounded action approaches, by contrast, leverage real-world feedback to adapt the simulator or policy, which often leads to more precise sim-real alignment\cite{Chebotar2019,Hanna2017,Ren2023}. These tend to perform better on specific tasks but depend on collecting sufficient real experience.

Figure~\ref{fig:adr_pipeline} through \ref{fig:adapt_sim} summarize illustrative results: learning in simulation with domain randomization enabled OpenAI’s robot hand to handle varied physical conditions, as shown in the ADR pipeline (Fig.~\ref{fig:adr_pipeline}); Tan et al.’s randomized locomotion policy succeeded on a real quadruped (Fig.~\ref{fig:quadruped}); and AdaptSim’s iterative adaptation dramatically improved real-world scooping (Fig.~\ref{fig:adapt_sim}). These include quantitative gains such as 2× longer drone flights (CAD2RL), >10× more successful manipulations (Dactyl), and faster, more energy-efficient quadruped gaits (Minitaur) in real-world settings. These examples underscore that combined strategies often work best: randomize broadly, then refine with targeted real data.

In practice, successful \simtoreal transfer often involves \emph{engineering} trade-offs. For example, one may choose which aspects of the simulator to increase fidelity (e.g.\ detailed actuator model) and which to randomize.There is evidence that focusing on the most significant discrepancies yields the most benefit. Future work may further integrate these techniques; e.g.\ meta-learning frameworks that automatically determine which parameters to randomize versus calibrate.

\paragraph{Meta-Evaluation and Strategy.}
While each method addresses a distinct element of the sim-to-real pipeline, their practical value often hinges on real-world feasibility, data availability, and task constraints. Observation-level techniques such as domain randomization offer strong generalization, especially for vision-based agents, but may suffer from limited realism or coverage gaps. Transition- and action-level transfer methods like system identification or grounded transformations yield higher precision but are inherently hardware-dependent.

Reward-level approaches represent the most fragile layer: while learned rewards, inverse RL, and preference-based methods reduce engineering burden, they are often unstable and require high-quality supervision signals. Across the reviewed literature, hybrid strategies emerge as the most effective: initial training with broad randomization followed by targeted adaptation (e.g., AdaptSim~\cite{Ren2023}) leads to the most consistent performance gains. For a structured overview of the trade-offs across sim-to-real methods—including data demands, generalization capabilities, and adaptation effort—we refer the reader to Appendix~\ref{app:benchmark}.

While Appendix~\ref{app:benchmark} provides a structured benchmark of current methods, future work should develop automated frameworks that dynamically select or combine sim-to-real techniques based on real-time discrepancies. Such systems would enable adaptive learning pipelines that identify which MDP components—observations, actions, transitions, or rewards—require intervention during deployment.

There is also a need for cross-domain benchmarks that assess transferability in more complex, long-horizon, or multi-agent tasks. Current evaluations often focus on isolated scenarios with clear metrics, while real-world applications involve greater ambiguity and variability.

Finally, integrating meta-learning or online adaptation into the sim-to-real loop may improve robustness by continuously updating simulation parameters or reward functions based on real-world feedback. Future research should thus aim for not only stronger methods but also their adaptive orchestration—a key challenge for scalable sim-to-real reinforcement learning.