\section{Cross-Domain Case Studies}
To ground the above concepts, we now highlight several case studies where sim-to-real techniques have enabled successful transfer in different application domains. These examples span robotic manipulation, drone flight, and legged locomotion – demonstrating both the commonalities and unique challenges of each domain. Each case study will mention the task, the approach used to bridge the gap, and key results achieved on real systems.

\subsection{Robotic Object Manipulation}
One of the early triumphs of sim-to-real in RL was in robotic object manipulation – specifically, having robot arms or hands handle objects using policies learned in simulation. A representative example is object grasping and localization. In the 2017 study by Tobin et al. (discussed earlier in Domain Randomization), the task was for a robotic arm to pick up objects (toys of various shapes) on a table using input from an RGB camera. The challenge was primarily vision-based: the robot needed to perceive the objects’ positions and plan grasps. The researchers trained a convolutional neural network to detect object locations by training entirely on synthetic images of a cluttered tabletop with random textures, lighting, and distractor objects (domain randomization). Even though the simulated images looked nothing like the real world individually, the neural network learned to focus on geometric cues and was robust to color/texture. When this network was used on the real robot’s camera input, it localized objects with ~1.5 cm accuracy and enabled reliable grasping in the real world – all without any real-image training. This was a landmark result showing that simulation-only training could produce a useful real-world perception system for manipulation.

Building on that, OpenAI’s Dactyl (2018) tackled an even harder manipulation problem: dexterous in-hand manipulation. In that setup, a five-fingered robot hand had to reorient a block (and later a Rubik’s Cube) in its fingers. They trained the policy in a MuJoCo simulator with extensive randomizations of both visuals and physics (frictions, object mass, joint gains, etc.). One especially important aspect was using an LSTM (recurrent network) in the policy to allow it to adapt online to the physics – essentially the policy would “feel” how heavy or slippery the object was and adjust its strategy. This can be seen as a form of implicit system identification happening inside the policy’s hidden state (a clever trick). After training on on the order of $10^8$ timesteps (with massive parallelization), the policy was deployed to the real Shadow Hand. The result: the hand could successfully spin and reposition the block in precise ways, achieving the goal orientations consistently. Later, they even demonstrated the hand solving a Rubik’s Cube – an extremely complex manipulation – using the same policy architecture (with some additional state estimation help). The success of Dactyl was a strong validation of the domain randomization approach for high-dimensional control. Figure 3 shows the real robotic hand from the Dactyl system manipulating a block, an outcome of this sim-to-real training.

Another manipulation example is assembly tasks, such as inserting pegs or screwing in bolts. These tasks require fine precision and often have contact-rich dynamics (which simulators struggle with). In 2019, researchers at NVIDIA (Mahler et al., 2019) showed that by using a high-fidelity simulator (Isaac Gym) and randomizing friction and part tolerances, a policy could learn to insert a peg into a hole in sim and then perform it on a real robot arm with high success. They also leveraged domain adaptation by using depth camera inputs (which are easier to match between sim and real than RGB images). Depth images from simulation are quite comparable to real depth sensor data, so the gap was smaller; in some sense, choosing the right sensing modality (depth over RGB) is also a strategy to ease sim-to-real.

To summarize the manipulation domain: Visual randomization has been critical for perception-driven tasks, and dynamics randomization plus recurrent policies or system ID has been important for contact-rich tasks. These strategies enabled policies trained in simulation to achieve real-world feats like multi-object grasping and dexterous hand manipulation, which are significant milestones in robotics. Even so, challenges remain, such as more complex object interactions, generalizing to novel objects not seen in simulation, and long-horizon manipulation sequences. Current research is exploring combining imitation learning from a few real demos with simulation training to handle those cases (so that the policy gets a hint of real-world behavior to anchor it).

\subsection{Drone Navigation (UAV Flight)}
Unmanned aerial vehicles (UAVs or drones) present a compelling application for sim-to-real RL because crashing a real drone during learning is very costly, yet drones operate in highly dynamic environments where a robust policy is needed. The case study of CAD2RL by Sadeghi and Levine (2016) is instructive. They wanted a drone to perform collision avoidance in indoor hallways using only a single camera image as input (no fancy laser sensors). They trained a deep RL policy in a 3D simulator that generated random hallway textures and layouts – essentially the drone learned to react to visual input to avoid walls. Crucially, the network was never shown a real image during training, only the randomized simulated images. When they flew the policy on a real drone (a small quadrotor with an onboard camera), it was able to traverse real hallways and avoid obstacles for hundreds of meters of flight, all without collisions. This zero-shot transfer was achieved primarily by domain randomization of the visual domain (wall colors, pictures on the wall, floor patterns, lighting, etc.), so the drone didn’t overfit to any particular appearance. The dynamics in that case were less of an issue because the policy was high-level (it output velocity commands); a low-level stabilizing controller was running on the drone to handle basic flight, which is common in such setups.

Another example in drone navigation is in outdoor flight or in simulated city environments. Researchers have used simulators like Microsoft AirSim or Gazebo to train drone policies for tasks like following roads or avoiding moving obstacles. Domain adaptation has been applied here by taking simulated images (which might, for instance, not perfectly capture the complexity of natural outdoor scenes) and using neural networks to enhance realism. For instance, one could train a network to translate simulated aerial images to look like real aerial images using satellite photo datasets. By doing so, an RL agent trained on the translated images would be more familiar with real visuals when deployed.

One of the challenges unique to drones is the dynamics: unlike many ground robots, drones have very fast dynamics and are quite sensitive to aerodynamic effects (propeller wash, wind) which are hard to simulate exactly. Some recent works (2019–2021) have started to incorporate dynamics randomization for drones – e.g., randomizing the mass of the drone, motor thrust curves, latency in control signals, and wind disturbances during training. Combined with PID controllers or adaptive control on the drone, this has improved success in transferring agile flight maneuvers from simulation to real quadrotors. For example, if a simulated drone can learn to do a flip or a fast turn in the presence of random gusts of wind and motor power variations, the resulting policy is more likely to work on a physical drone where those factors occur naturally.

In summary, for drone navigation, vision-based tasks have benefited hugely from visual domain randomization (as in CAD2RL), while highly dynamic tasks require careful system identification (tuning simulators for drone dynamics) and possibly domain randomization of dynamics. Safety is a big concern – unlike a robot arm that might just miss a grasp, a bad drone policy can crash the vehicle. So often a conservative approach is used: the RL policy might be given to the real drone only when a safety controller can take over if it diverges, or training might happen progressively (low-speed flight policies then increasing speed). This relates to future directions in safe RL, but it’s worth noting as a practical aspect of sim-to-real for UAVs.

\subsection{Legged Robot Locomotion}
Legged robots, such as bipeds or quadrupeds, have high degrees of freedom and complex contact dynamics with the ground. They are an excellent testbed for sim-to-real because falls or instability in the real world are potentially damaging, yet simulation of legged locomotion can be made quite realistic with modern physics engines. One prominent case study is deep RL for quadruped locomotion by Tan et al. (2018). In that work, the task was to make a quadruped robot (with four legs, similar to MIT’s Mini Cheetah or Unitree’s Laikago) learn to trot and gallop – gaits that require dynamic balance. They used a combination of system identification and domain randomization: first, they improved their simulator’s fidelity by identifying the robot’s motor model and sensor latency (the time delay between sending a command and the motor responding, and between an event and the sensor reading). This alone significantly narrowed the gap because a lot of control policies are sensitive to latency. Next, they applied dynamics randomization during training: they varied parameters like friction coefficients of the feet and ground, small variations in motor torque output, and they even added random pushes to the robot in simulation to teach it to recover balance. They also constrained the observation space to be something the real robot could reliably sense (e.g., using on-board inertial measurements rather than assuming access to global orientation). With these measures, they trained a policy (using PPO) in simulation that could produce fast trotting and galloping gaits.

The result was that when this policy was deployed on the real quadruped robot, it could execute the learned gaits and remain stable, without any additional training on the robot. The robot achieved a trot at around 0.5 m/s and a galloping gait faster than that, which was quite an achievement at the time for learned controllers. The sim-to-real transfer here crucially depended on both identification (to get the simulator close, e.g. matching the real motor dynamics) and randomization (to account for remaining discrepancies, like a slightly different friction on the lab floor versus the sim). If they had trained only on a nominal simulator, small unmodeled effects could have led to the robot tripping or oscillating. Because the policy had experience with those perturbations (random pushes, etc.), it was robust.

Following this, many other works came out applying similar ideas to different robots. For instance, Hwangbo et al. (2019) trained a bipedal robot to jump and run using an accurate simulator and randomizing ground compliance (how springy the ground is), which let the robot handle both hard flooring and softer mats in reality. The theme is consistent: identify what matters, randomize the rest. Another interesting case was by Muratore et al. (2021) on a ball-in-cup task with a robot arm (a task where the robot must swing a cup to catch a ball on a string). They used dynamics randomization with adaptive feedback, meaning the policy had parameters that could adjust based on observed behavior differences, effectively learning a form of online system ID (this blurs into meta-learning territory). The policy learned to compensate for slightly different string lengths and ball weights between sim and real, successfully performing the trick on real hardware.

For legged locomotion, another challenge is often the computational cost: simulation of many-legged robots with contact can be slow. This is where tools like Isaac Gym’s massive parallelism have helped. Researchers have shown that you can train a policy for a simple biped to walk in just a few minutes using thousands of parallel simulations – a case of brute force enabled by GPUs. Once trained, transfer still needs the above techniques, but the training turnaround is faster.

To sum up the locomotion case: It highlights the importance of physics-side alignment. Visual realism is usually not an issue (these policies often don’t rely on external cameras, but on internal sensors like joint encoders, IMUs, etc.). Instead, it’s all about dynamics: masses, friction, delays, and unexpected forces. By addressing those through system identification and domain randomization, researchers have achieved robust locomotion controllers via deep RL that operate on real legged robots – something that even a few years ago was very difficult. Still, open problems include handling environment changes (e.g., different terrains like slippery ice or tall grass – one can randomize terrain properties to an extent, but extreme changes may need on-the-fly adaptation) and multi-contact scenarios (like a humanoid climbing or using

\subsection{(continued) Legged Locomotion (cont.)}
...multi-contact scenarios. For example, getting a bipedal humanoid to not only walk but also climb or use its arms for support involves complex interactions that are hard to simulate perfectly. Current sim-to-real techniques have yet to fully conquer these extreme cases, and they remain active research problems.