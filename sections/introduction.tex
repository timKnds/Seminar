\section{Introduction}
Reinforcement Learning (RL) has achieved remarkable results in simulated environments (e.g.\ video games, simulated robotics) but often fails to generalize to real-world systems due to the \emph{\simtoreal gap}. Simulation can provide abundant and safe data, yet differences in visual perception, physical dynamics, and task specifics cause learned policies to degrade in reality {\cite{Tobin2017,Sadeghi2017}}. For instance, a policy that navigates hallways with randomized CAD\footnote{CAD = Computer Aided Desing} renderings can transfer to a real drone (CAD2RL {\cite{Sadeghi2017}}), and a humanoid robot hand can solve a Rubik’s cube in reality after training with Automatic Domain Randomization (ADR) in simulation {\cite{Akkaya2019}}. Nevertheless, open challenges remain: how to systematically reconcile each aspect of the Markov Decision Process (observations, actions, transitions, rewards) between sim and real. 

This paper revises and streamlines the literature through the Observation–Action–Transition–Reward (OATR) lens. We define each MDP\footnote{MDP = Markov Decision Process} component and describe how discrepancies in that component contribute to the transfer challenge. We then review \simtoreal methods aligned to each OATR category, emphasizing practical examples and empirical findings (e.g.\ Fig.~\ref{fig:adr_pipeline}, Fig.~\ref{fig:adapt_sim}). In each section, we critically evaluate techniques (such as domain randomization versus system identification for dynamics) and highlight gaps. Our goal is to provide a clear, concise master’s-level overview: by focusing on OATR, readers can identify which aspects of sim or real must be bridged and which methods apply. The remainder is organized into Observation-, Action-, Transition-, and Reward-focused transfer.
