\section{Introduction}
Reinforcement Learning (RL) is a machine learning paradigm in which an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards. Through trial-and-error, the agent improves its policy to maximize cumulative rewards. In recent years, deep RL has solved complex tasks in sumulation - from playing video games to controlling simulated robots - showcasing the protential of automomous learning. However, applying these learned policies on pysical systems presents a significant challenge known as the sim-to-real gap. Simulated training is appealing because collecting real-world data can be costly, slow, or unsafe, whereas simulations provide virtually unlimited, fast data in a safe setting. Yet differences between simulated and real physics, sensors, and visuals often cause a policy that works well in simulation to fail on the real hardware. Bridging this sim-to-real gap has become a crucial research focus in fields like robotics and atonomous vehicles.To address the reality gap, researchers have developed a spectrum of transfer techniques.

This paper revises the core sections of sim-to-real transfer in RL by organizing the discussion around the fundamental components of an RL problem, following the Observation–Action–Transition–Reward (OATR) framework proposed by recent work. We first introduce RL fundamentals in terms of these four components, highlighting how each contributes to the sim-to-real discrepancy. We then provide a structured review of sim-to-real transfer techniques, explicitly mapping each method to the OATR element it targets. We preserve key references and seminal examples from prior research – including CAD2RL for drone navigation, OpenAI’s Dactyl and ADR (Automatic Domain Randomization) for robotic manipulation, and Tan et al.’s quadruped locomotion – to illustrate each category of method.